"""Optimizer
Either write a gradient descent here, wrappers for optimizers in jax.experimental.
Can also just put everything here and optimize the quantity lnpost = ln(prob) + ln(prior)

Check this out for how it uses the Adam optimizer https://roberttlange.github.io/posts/2020/03/blog-post-10/.

Also see https://roberttlange.github.io/posts/2020/03/blog-post-10/.

Or we can just do this in a notebook...
"""